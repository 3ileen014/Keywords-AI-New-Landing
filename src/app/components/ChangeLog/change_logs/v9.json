{
    "date": "July 27, 2024",
    "Title": "New feature - Caches!",
    "snapshot": "caches.jpg",
    "introduction": "Supercharge Your LLM Calls! Our new Caches feature allows you to store and reuse LLM responses, eliminating redundant API calls. \n\n This smart caching system optimizes your AI performance by delivering instant responses, reducing costs, and ensuring consistent, high-quality outputs.",
    "sections": {
      "New": [
        {"tag": "API", "description": "Added a new endpoint to update logs. You could update logs with metadata and notes using the PATCH method on the /api/request-logs/batch-update/ endpoint."}

      ],
      "Improved": [
        {"tag": "Dashboard", "description": "Improved the loading time of the 30-day graph in the Dashboard."}
      ],
      "Fixed": [
        {"tag": "Users", "description": "Fixed the bug where Name and Email cannot be empty."}
      ]
    }
  }
  